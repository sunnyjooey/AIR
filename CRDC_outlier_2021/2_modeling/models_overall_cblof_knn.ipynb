{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "import functools\n",
    "import pickle\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models import cblof\n",
    "from keras import losses\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "\n",
    "# read in and subset to region (nb)\n",
    "dat_dir = 'I:/NCES/NCES_Dev/sunjoo_LEE_MOVE/CRDC_outlier_2021_2/0_processed_data/'\n",
    "the_data_file = '{}crdc_prepped_formod.csv'.format(dat_dir)\n",
    "df = pd.read_csv(the_data_file)\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module: columns in module\n",
    "with open('{}mod_col.txt'.format(dat_dir), 'rb') as handle:\n",
    "    new_mod_dict = json.load(handle)\n",
    "cols_to_run = [item for sublist in new_mod_dict.values() for item in sublist]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[cols_to_run]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "clfs = {'CBLOF': cblof.CBLOF()}\n",
    "\n",
    "# params only for cblof\n",
    "param_grid = {'CBLOF': {'n_clusters': [20],\n",
    "                        'contamination': [0.01],\n",
    "                        'clustering_estimator': [KMeans(algorithm='full', copy_x=True, init='k-means++', max_iter=1000,\n",
    "                                                        n_clusters=8, n_init=10, n_jobs=None, precompute_distances='auto',\n",
    "                                                        random_state=None, tol=0.0001, verbose=0)],\n",
    "                        'alpha': [0.9],\n",
    "                        'beta': [5],\n",
    "                        'use_weights': [False],\n",
    "                        'check_estimator': [False]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all model specs\n"
     ]
    }
   ],
   "source": [
    "# create list of params to go through and update during modeling -- this has all the specifications for each model!\n",
    "all_clf_mods = []\n",
    "clf_name = 'CBLOF'\n",
    "parameter_values = param_grid[clf_name] #creates a set of params for each combination in hyper-param lists\n",
    "for p in ParameterGrid(parameter_values):\n",
    "    seed = random.randint(0, 1000000)\n",
    "    clf_param_id = clf_name + '_' + str(seed)\n",
    "    clf_param_info = {clf_param_id: {}} #create dictionary to keep track of everything about the clf and particular params\n",
    "    clf_param_info[clf_param_id]['params'] = p\n",
    "    clf_param_info[clf_param_id]['clf'] = clf_name\n",
    "    clf_param_info[clf_param_id]['seed'] = seed\n",
    "    clf_param_info[clf_param_id]['modules_done'] = []\n",
    "    clf_param_info[clf_param_id]['ten_fold_done'] = 0\n",
    "    all_clf_mods.append(clf_param_info)\n",
    "\n",
    "\n",
    "mod_dir = 'I:/NCES/NCES_Dev/sunjoo_LEE_MOVE/CRDC_outlier_2021_2/2_modeling/'\n",
    "\n",
    "if not os.path.isdir('{}models/'.format(mod_dir)):\n",
    "    os.mkdir('{}models/'.format(mod_dir))\n",
    "    print('Made models dir')\n",
    "\n",
    "if not os.path.isdir('{}results/'.format(mod_dir)):\n",
    "    os.mkdir('{}results/'.format(mod_dir))    \n",
    "    print('Made results dir')\n",
    "\n",
    "# this is to prevent accidentally overwriting models list -- manually go and delete it first if you want to replace it\n",
    "if not os.path.isfile('{}models/all_clf_mods.pickle'.format(mod_dir)):    \n",
    "    with open('{}models/all_clf_mods.pickle'.format(mod_dir), 'wb') as handle:\n",
    "        pickle.dump(all_clf_mods, handle)\n",
    "    print('Saved all model specs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# do not transform ratios and indicators\n",
    "cols = df.columns\n",
    "all_transform_cols = [col for col in cols if '_ind' not in col and '_ratio' not in col and '_mean' not in col and col!='pov_per_5-17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for some variables\n",
    "hs_15 = ['sch_grade_09', 'sch_grade_10', 'sch_grade_11', 'sch_grade_12', 'sch_ugdetail_hs'] #each 1.15 weight\n",
    "stat_20 = ['sch_status_sped','sch_status_charter','sch_status_magnet', 'sch_altfocus_pre_mean'] #each 1.2 weight\n",
    "imp_25 = ['sch_grade_ps','sch_altfocus_post_mean','tot_enrl','pov_per_5-17'] #each 1.25 weight\n",
    "d1 = {key:1.15 for key in hs_15}\n",
    "d2 = {key:1.2 for key in stat_20}\n",
    "d3 = {key:1.25 for key in imp_25}\n",
    "col_weights = {**d1, **d2, **d3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CBLOF_575580\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# run, baby, run\n",
    "for clf_i, mod_dict in enumerate(all_clf_mods): #this is a list of dictionaries\n",
    "    for clf_param_id, info_dict in mod_dict.items(): #there is only one\n",
    "       \n",
    "        print('')\n",
    "        print(clf_param_id)\n",
    "        clf_name = info_dict['clf'] #COPOD\n",
    "        p = info_dict['params'] #dict of params\n",
    "        clf = clfs[clf_name] #base classifier\n",
    "        # set params for classifier\n",
    "        clf.set_params(**p)\n",
    "        if clf_name in ['CBLOF','IForest','AUTO_ENC']:\n",
    "            clf.set_params(random_state=info_dict['seed']) #use same seed\n",
    "            if clf_name == 'CBLOF':\n",
    "                #set params for KMeans in CBLOF (doesn't seem to happen automatically)\n",
    "                clf.clustering_estimator.set_params(**{'n_clusters': info_dict['params']['n_clusters'], 'random_state':info_dict['seed']})\t\n",
    "\n",
    "            # divide into 10\n",
    "            random.seed(info_dict['seed'])\n",
    "            samp = [random.randint(1,10) for x in range(df.shape[0])]\n",
    "            \n",
    "            df_scores = pd.DataFrame()\n",
    "            df_scores['score'] = [0] * df.shape[0]\n",
    "\n",
    "            # already done i\n",
    "            done_i = info_dict['ten_fold_done']\n",
    "            for i in range(done_i+1, 11):\n",
    "                # divide into training and testing\n",
    "                idx = [ii for ii,e in enumerate(samp) if e!=i]\n",
    "                idx_t = [ii for ii in range(df.shape[0]) if ii not in idx]\n",
    "\n",
    "                # get cols to transform\n",
    "                transform_cols = [col for col in df.columns if col in all_transform_cols]\n",
    "                col_transformer = ColumnTransformer(\n",
    "                    transformers=[('ss', StandardScaler(), transform_cols)],\n",
    "                    remainder='passthrough',\n",
    "                    transformer_weights=col_weights\n",
    "                    )\n",
    "\n",
    "                # train on 9/10, fit on 1/10\n",
    "                X_train = df.iloc[idx,:]\n",
    "                X_train_transformed = col_transformer.fit_transform(X_train)\n",
    "                del X_train\n",
    "                X_test = df.iloc[idx_t,:]\n",
    "                X_test_transformed = col_transformer.transform(X_test)\n",
    "                del X_test\n",
    "                # train\n",
    "                clf.fit(X_train_transformed)\n",
    "\n",
    "                # get outlier scores for last tenth of data\n",
    "                y_test_scores = clf.decision_function(X_test_transformed)  # outlier scores\n",
    "                df_scores.iloc[idx_t, 0] = y_test_scores\n",
    "                print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('{}results/df_scores.csv'.format(mod_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97564, 828)\n"
     ]
    }
   ],
   "source": [
    "geo = pd.read_csv('{}ocr_region.csv'.format(dat_dir))\n",
    "raw = pd.read_csv('{}crdc_prepped.csv'.format(dat_dir), dtype={'combokey':str, 'leaid':str})\n",
    "print(raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo['state'] = geo['state'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97564, 829)\n"
     ]
    }
   ],
   "source": [
    "raw = pd.merge(raw, geo, how='left', on='state')\n",
    "df = pd.concat([df, raw['region']], axis=1)\n",
    "print(raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97564, 825)\n"
     ]
    }
   ],
   "source": [
    "df = df[df['region'].notna()]\n",
    "del raw\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# num neighbors\n",
    "N = 20\n",
    "\n",
    "regs = [\"Atlanta\", \"Seattle\", \"Denver\", \"Kansas City\", \"San Fransisco\", \"Boston\", \"Philadelphia\", \"Chicago\", \"Dallas\", \"Cleveland\", \"New York\", \"DC\"]\n",
    "for reg in regs:\n",
    "    d = df[df['region'] == reg]\n",
    "    del d['region'] \n",
    "    # make transformer\n",
    "    transform_cols = [col for col in d.columns if col in all_transform_cols]\n",
    "    col_transformer = ColumnTransformer(\n",
    "                        transformers=[('ss', StandardScaler(), transform_cols)],\n",
    "                        remainder='passthrough',\n",
    "                        transformer_weights=col_weights\n",
    "                        )\n",
    "    # fit knn, get top 100 for each\n",
    "    X_train_transformed = col_transformer.fit_transform(d)\n",
    "    neigh = neighbors.NearestNeighbors(n_neighbors=N, n_jobs=-1)\n",
    "    neigh.fit(X_train_transformed)\n",
    "    neighs = neigh.kneighbors()\n",
    "    d = pd.DataFrame(neighs[1])\n",
    "    d.to_csv('{}results/neigh_{}.csv'.format(mod_dir, reg), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
